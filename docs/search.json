[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CASA0023 Learning Diary",
    "section": "",
    "text": "Welcome\nHello!\nWelcome to my learning diary!"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "9  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "CASA0023 Learning Diary",
    "section": "About me",
    "text": "About me\n\nI am Yuxi Zheng from Hubei, China, currently studying for an MSc in Urban Spatial Science at University College London (UCL). I completed my undergraduate studies in Landscape Architecture at Beijing Forestry University (BJFU). I am interested in the application of spatial data analysis in urban ecology. I am passionate about contributing to the protection and improvement of our planet’s ecology by utilizing a diverse array of spatial data for analysis. With a focus on harnessing the power of technology and data-driven insights, I aim to drive positive change and sustainable solutions for our environment.  \n\n\n\n\n\n\n\n\n\n\nContact Information\nEmail: ucfnyz0@ucl.ac.uk / yuxi9662@qq.com\nPhone numbers: +44 07514272307 / +86 13972521203"
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "CASA0023 Learning Diary",
    "section": "About this book",
    "text": "About this book\n\nIn Spring 2023, while pursuing my studies at UCL, I authored a book that formed a significant part of my assessment for the course CASA0023, focused on Remote Sensing Cities and Environments. This book is a comprehensive guide on remote sensing principles, their applications, and data analysis using R and Google Earth Engine. I used Quarto Book to edit the book and made sure that all the codes are available on my Github repository as open-source for interested readers.  \n\n\nThe book presents a range of topics in the field of remote sensing, including satellite image acquisition and processing, image classification, and change detection. I also discuss various applications of remote sensing in environmental studies, urban planning, and disaster management. To provide readers with hands-on experience, the book includes practical exercises and step-by-step instructions for working with satellite imagery in R and Google Earth Engine."
  },
  {
    "objectID": "index.html#ackownledgement",
    "href": "index.html#ackownledgement",
    "title": "CASA0023 Learning Diary",
    "section": "Ackownledgement",
    "text": "Ackownledgement\nMany thanks to Andrew MacLachlan for his help with this book！"
  },
  {
    "objectID": "week5.html",
    "href": "week5.html",
    "title": "\n5  week5\n",
    "section": "",
    "text": "Google Earth Engine Basic Knowledge"
  },
  {
    "objectID": "week5.html#summary",
    "href": "week5.html#summary",
    "title": "\n5  week5\n",
    "section": "\n5.1 Summary",
    "text": "5.1 Summary\n\n5.1.1 Google Earth Engine Introduction\n\nGoogle Earth Engine is a cloud-based platform for processing and analyzing geospatial data. It combines vast repositories of satellite imagery and other geospatial data with powerful processing tools to enable users to perform complex analysis and generate dynamic visualizations of the Earth’s surface. With its ability to process petabytes of data quickly and efficiently, Google Earth Engine has become an essential tool for scientists, researchers, policymakers, and anyone who is interested in understanding our planet and its environment. Its innovative features and accessibility have made it an important player in the fields of ecology, conservation, urban planning, and disaster response. Google Earth Engine is a game-changer in the world of geospatial analysis and has the potential to transform the way we understand and interact with the world around us.  \n\n\n5.1.2 Difference between Client and Server\nDifference\n\n\n\n\n\n\n\nAspects\nClient\nServer\n\n\n\nFunction\nInteract and visualize geospatial data\nProvide computational power and storage\n\n\nUsage\nBasic analysis and visualization\nAccess and process data programmatically\n\n\nInterface\nWeb-based interactive\nNo graphical interface\n\n\nProcessing\nClient-side\nServer-side with distributed computing\n\n\n\nClient\n\nHere are some examples of projects where GEE applications have been implemented.  \n\nList of awesome GEE apps\nAir quality example\nNDVI slider example\n(Source: Andrew Maclachlan)\nServer\n\nGEE Server provides APIs for developers to access and process data programmatically.  \n\n\n5.1.3 GEE API\n\n5.1.3.1 JavaScript\n\nGee coding based on JavaScript, this is almost all of the Javascript you need to know:  \n\nvar number = 1\n\nvar string = 'Hello, World!'\n\nvar list = [1.23, 8, -3]\nprint(list[2])\n\nvar dictionary = {\n  a: 'Hello',\n  b: 10,\n  c: 0.1343,\n  d: list\n}\n\nprint(dictionary.b)\nprint(number, string, list, dictionary)\n(Source: Andrew Maclachlan)\n\nSome of the basic geospatial data processing steps in GEE and the functions that go into them are as follows:  \n\n\n\n\n\n\n\nGeospatial Processing Steps\nGeospatial Processing Functions\n\n\n\nImage\nband math, clip, convolution, neighborhood, selection\n\n\nImage Collection\nmap, aggregate, filter\n\n\nFeature\nbuffer, centroid, intersection, union, transform\n\n\nFeature Collection\naggregate, filter, flatten, merge, sort\n\n\nFilter\nby bounds, within distance, date, day-of-year, metadata\n\n\nReducer\nmean, linearRegression, percentile, histogram\n\n\nJoin\nsimple, inner, outer, inverted\n\n\nKernel\nsquare, circle, gaussian, sobel, kirsch\n\n\nExport\nto geotiff, to video, to TensorFlow, to map tiles\n\n\nMachine Learning\nCART, random forests, baes, SVM, kmeans, cobweb\n\n\nProjection\ntransform, translate, scale\n\n\n\nCommon basic operations and syntax. Source: youtube.com/watch?v=I-wFYm4Hnhg\n\n5.1.3.2 GEE Code Editor\n\n\n\n\nGEE Code Editor. Source: code.earthengine.google.com/"
  },
  {
    "objectID": "week5.html#application",
    "href": "week5.html#application",
    "title": "\n5  week5\n",
    "section": "\n5.2 Application",
    "text": "5.2 Application\n\n5.2.1 GEE Basic operations\n\nHere are some of the basic operations I perform in gee, including loading image data, scaling images, mosaicing images, clipping images, and texture analysis, pca analysis, etc.  \n\nmy code link: https://code.earthengine.google.com/2d91d811e683d25ff8f3e71a16a8046c\nA series of image enhancement processes\n\n\n\n\nImageEnhance. Source: Yuxi\n\n\n\n\nclipping images\n\n\n\n\nImageEnhance. Source: Yuxi\n\n\n\n\ntexture analysis\n\nGLCM texture analysis describes the texture information in an image by calculating a matrix of co-occurrence of the grey levels of pixels around the same pixel. This texture information can in turn be used to reflect the surface texture, shape, size and other features of the feature, thus providing the base data for subsequent analysis and applications.  \n\n\n\n\n\nImageEnhance. Source: Yuxi\n\n\n\n\npca analysis\n\nPrincipal component analysis (PCA) was performed on a GLCM (grey scale co-occurrence matrix) image of India using Google Earth Engine (GEE) with the following process:  \n\n\n\n\n\nImageEnhance. Source: Yuxi\n\n\n\n\n\nBy performing PCA on a GLCM image of India, we can identify the principal components that explain the most variance in the image. These components can then be used to create a new image that captures the essential features of the original image while removing any noise or redundant information.And it helps to reduce the dimensionality of large datasets.  \n\n\n\n\n\nImageEnhance. Source: Yuxi\n\n\n\n\n\n5.2.2 Examples\nTexture Analysis\n\nLi et al. (2019) used GEE to perform texture analysis on Sentinel-2 imagery for mapping vegetation cover in the Tibetan Plateau. They applied gray-level co-occurrence matrix (GLCM) and local binary patterns (LBP) to extract texture features from the satellite imagery. Then, they used a random forest algorithm to classify the vegetation cover based on these texture features. The results showed that texture analysis improved the classification accuracy compared to using spectral bands alone. This study suggests that texture analysis can be a useful tool for mapping vegetation cover in high-altitude areas.  \n\nPrincipal Component Analysis\n\nGao et al. (2019) used GEE to perform PCA on Sentinel-2 imagery for mapping land cover in the Three Gorges Reservoir Area, China. They applied PCA to reduce the dimensionality of the multivariate data and extract the most important information. Then, they used a support vector machine (SVM) algorithm to classify the land cover based on the principal components. The results showed that PCA improved the classification accuracy compared to using all the spectral bands. This study suggests that PCA can be a useful tool for mapping land cover in complex areas with diverse land cover types.  \n\nConclusion\n\nIn conclusion, texture analysis and PCA are powerful tools for analyzing satellite imagery in urban or ecological environments. Texture analysis can extract spatial patterns and improve classification accuracy, while PCA can reduce dimensionality and extract the most important information. By using these techniques, researchers can better understand and manage urban or ecological environments, ultimately contributing to the protection and preservation of these areas."
  },
  {
    "objectID": "week5.html#reflection",
    "href": "week5.html#reflection",
    "title": "\n5  week5\n",
    "section": "\n5.3 Reflection",
    "text": "5.3 Reflection\n\nIn learning to use Google Earth Engine (GEE), I have found that it has many advantages. Firstly, GEE provides easy access to data and processing tools, making it easy for users to work with remote sensing data in the cloud without having to download and store large amounts of data locally. Secondly, GEE provides a wealth of remote sensing data, tools and algorithms that users can write code in JavaScript to use for data analysis and visualisation. In addition, GEE’s code is sharable, which allows users to share their code and collaborate with others on data analysis and model building. Finally, GEE supports large-scale data processing and distributed computing, allowing users to work with large amounts of data more efficiently.  \n\n\nDuring my studies, I learnt a lot about GEE operations, some of which include data acquisition, data pre-processing, image analysis and visualisation. For example, in data acquisition, I learnt how to acquire remote sensing data such as Sentinel-2, Landsat and MODIS from the GEE data repository and load them into code for analysis. In data pre-processing, I learnt how to perform pre-processing operations such as image cropping, image stitching, image reprojection and image de-clouding. In image analysis, I learnt how to perform operations such as remote sensing classification, texture analysis and principal component analysis.  \n\n\nIn practice, I found that sometimes the code execution did not work or the analysis results were not as expected, when I would try to review the code and data to find out what might have gone wrong, and debug and improve it. At the same time, I also think about why things are going wrong and try to understand the root of the problem so that I can better avoid such problems in the future.  \n\n\nI agree that GEE is a powerful tool for analyzing remote sensing data, and it offers many advantages over traditional methods such as R or SNAP. However, I also believe that there are certain limitations to GEE that need to be addressed. For instance, while GEE can handle large datasets, it requires a stable internet connection, which may not be available in some areas. Additionally, GEE is a cloud-based platform, which means that the user has little control over the hardware and software used for processing the data. This could potentially lead to issues with data security and privacy.  \n\nLi, Z., Feng, X., Zhang, L., Lv, G., Zhou, X., & Liu, Q. (2019). Mapping Vegetation Cover in the Tibetan Plateau Using Texture Analysis of Sentinel-2 Imagery. Remote Sensing, 11(15), 1817.\nGao, Y., Zhang, Y., He, M., & Wu, J. (2019). Mapping Land Cover in the Three Gorges Reservoir Area with Sentinel-2 Imagery Using Principal Component Analysis and Support Vector Machine. Remote Sensing, 11(23), 2829."
  },
  {
    "objectID": "week6.html",
    "href": "week6.html",
    "title": "\n6  week6\n",
    "section": "",
    "text": "Classification I"
  },
  {
    "objectID": "week6.html#summary",
    "href": "week6.html#summary",
    "title": "\n6  week6\n",
    "section": "\n6.1 Summary",
    "text": "6.1 Summary\n\n6.1.1 Basics of remote sensing classification\n\nBefore learning about remote sensing classification, we need to have some basic knowledge of machine learning.  \n\n\n\n\n\nClassificationBasic. Source: Yuxi\n\n\n\n\n\n6.1.1.1 Decision Trees\n\nA decision tree is a flowchart-like structure that represents different possible decisions and their possible consequences. In machine learning, decision trees can be used for classification or regression tasks.  \n\n\nDecision tree classification involves using a decision tree to classify data points into different categories based on their features. The decision tree algorithm uses the features of the data to create a set of rules that can be used to classify new data points.  \n\n\nDecision tree regression, on the other hand, involves using a decision tree to predict a numerical value for a target variable based on the values of several input variables. The decision tree algorithm creates a model that maps input variables to a predicted output value.  \n\n\nIn both cases, decision trees are trained on a set of labeled data, meaning data with known classifications or target values, and then used to predict classifications or target values for new data points. The quality of the model is evaluated based on its accuracy in predicting classifications or target values for the test data.  \n\nClassification\n\n\n\n\nDecisionTreeClassification. Source: scikit-learn\n\n\n\n\n\n\nDecisionTreeClassification. Source: scikit-learn\n\n\n\n\nRegression\n\n\n\n\nDecisionTreeRegression. Source: scikit-learn\n\n\n\n\n\n6.1.1.2 Random Forest\n\nRandom Forest builds multiple decision trees and combines their predictions to improve accuracy and reduce overfitting. It is particularly useful for high-dimensional datasets. The algorithm selects a random subset of the training data and a random subset of the input features for each decision tree, and then combines their predictions through a majority vote or average.  \n\n\n\n\n\nRandomForest. Source: IBM\n\n\n\n\n\n6.1.1.3 Support Vector Machine\n\nSupport Vector Machine (SVM) works by finding the optimal hyperplane that maximally separates the data into different classes. The algorithm selects the hyperplane with the largest margin between the classes, which is achieved by finding the support vectors, or the data points closest to the decision boundary. SVM can handle non-linearly separable data by using kernel functions to map the data to a higher dimensional space.  \n\n\n\n\n\nSupportVectorMachine. Source: SVM\n\n\n\n\n\n6.1.2 Image classification\n\nHImage classification is the task of categorizing an image into a predefined set of classes. This is typically done using machine learning algorithms, such as convolutional neural networks (CNNs), which learn to recognize patterns in images and make predictions based on those patterns. There are several types of image classification methods, including supervised learning, unsupervised learning.  \n\n\n\n\n\nImageClassification. Source: youtube\n\n\n\n\n\n6.1.2.1 Related examples\nclassify land-use types\nZhang et al. (2019) applies image classification to urban land-use classification using remote sensing data. The authors use a combination of deep convolutional neural networks (CNNs) and extreme learning machines (ELMs) to classify land-use types in urban areas. The results show that the proposed method outperforms traditional classification methods in terms of accuracy, and can be used for large-scale land-use mapping and urban planning.\nmonitor land cover change\nLu et al. (2019) uses an object-based convolutional neural network (OB-CNN) to classify land-use types in urban areas using multi-source remote sensing data. The authors demonstrate that the OB-CNN method is effective in capturing the spatial context and spectral information of urban land-use, resulting in higher classification accuracy compared to traditional methods. The results can be used for urban planning and environmental monitoring.\ntrack environmental changes\nLiu et al. (2020) applies image classification to monitor urban forest cover change using remote sensing data. The authors use an OB-CNN method to classify urban forest cover and track changes over time. The results demonstrate the potential of OB-CNN for accurate and efficient monitoring of urban forest cover change, which can be used for urban planning and environmental conservation.\nConclusion\nIn conclusion, image classification can be applied to remote sensing urban and environmental studies to classify land-use types, monitor land cover change, and track environmental changes over time. The use of deep learning methods, such as CNNs and OB-CNN, can improve classification accuracy and efficiency. These results have important implications for urban planning and environmental management."
  },
  {
    "objectID": "week6.html#application",
    "href": "week6.html#application",
    "title": "\n6  week6\n",
    "section": "\n6.2 Application",
    "text": "6.2 Application\n\n6.2.1 Classification Workflow\n\nHere is my application of supervised classification of remotely sensed images in GEE using random forests, and my procedure is as follows:  \n\nmy code link: https://code.earthengine.google.com/007b0dfe5b99cad35fd48c9adc919eca\n\n\n\n\nClassificationExampleI. Source: Yuxi\n\n\n\n\n\n6.2.2 Classification Output\n\nThe results after random forest classification and pixel training and classification, respectively, are obtained as follows:  \n\n\n\n\n\nRF. Source: Yuxi\n\n\n\n\n\n\n\n\nRF_Piexl. Source: Yuxi"
  },
  {
    "objectID": "week6.html#reflection",
    "href": "week6.html#reflection",
    "title": "\n6  week6\n",
    "section": "\n6.3 Reflection",
    "text": "6.3 Reflection\n\nAfter learning about the basics of remote sensing classification using machine learning, as well as image classification methods and their applications in remote sensing, urban and environmental domains, I have gained valuable knowledge and insights.  \n\n\nFirstly, I have learned about the importance of feature selection and extraction in remote sensing classification. Different features, such as spectral, spatial, and textural features, can be used to improve the accuracy of classification results. Machine learning algorithms, such as random forests and support vector machines, can be used to classify remote sensing data based on these features.  \n\n\nSecondly,I have learned about the applications of image classification in various fields, such as urban planning, land use and land cover mapping, and environmental monitoring. These applications have significant implications for understanding and addressing various environmental and societal issues.  \n\n\nMoreover, I have gained an understanding of the challenges involved in remote sensing classification, such as the presence of mixed pixels, noise, and the need for ground truth data. These challenges require careful consideration and evaluation when choosing and implementing classification methods. I have reflected on the limitations and biases that can exist in remote sensing data and how these can impact classification accuracy. It is important to be aware of these limitations and to critically evaluate the data and methods used in remote sensing classification.  \n\nReferences:\nZhang, W., Jiang, H., & Wu, S. (2019). An urban land-use classification method based on deep convolutional neural network and extreme learning machine. International Journal of Remote Sensing, 40(6), 2236-2258.\nLu, L., Li, Y., Fu, X., & Liu, X. (2019). An object-based convolutional neural network for land-use classification using multi-source remote sensing data. Remote Sensing, 11(8), 888.\nLiu, X., Jiao, L., & Guo, Y. (2020). Monitoring urban forest cover change based on object-based convolutional neural network. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 13, 2652-2660."
  },
  {
    "objectID": "week7.html",
    "href": "week7.html",
    "title": "\n7  week7\n",
    "section": "",
    "text": "Classification II"
  },
  {
    "objectID": "week7.html#summary",
    "href": "week7.html#summary",
    "title": "\n7  week7\n",
    "section": "\n7.1 Summary",
    "text": "7.1 Summary\n\n7.1.1 Basics of remote sensing classification\n\nThere are three categories of SubPixel Analysis Object-Based Image Analysis Superpixel, which are distinguished by the following diagram：\n\n\n\n\n\n\n\n\n\nFeature\nSubPixel Analysis\nObject-Based Image Analysis\nSuperpixel\n\n\n\nDefinition\nAnalyzes and classifies each pixel into spectral subunits.\nGroups adjacent pixels into objects and analyzes them based on their properties.\nClusters adjacent pixels with similar characteristics to create more meaningful units.\n\n\nMethod\nEstimates spectral values of subunits using math models and algorithms.\nSegments image into objects, extracts features, and classifies them.\nGenerates superpixels, extracts features, and classifies them.\n\n\nWorkflow\nExtraction, classification, mapping.\nSegmentation, feature extraction, object classification, accuracy assessment.\nPre-processing, superpixel generation, feature extraction, classification.\n\n\nAdvantages\nDetects sub-pixel spectral variations, improves classification accuracy.\nCaptures spatial and contextual information, reduces “salt and pepper” effect.\nReduces data redundancy, simplifies analysis, preserves spatial resolution.\n\n\nDisadvantages\nSensitive to noise, requires high computational resources.\nCan result in over- or under-segmentation, requires careful selection of parameters.\nCan over- or under-segment, resulting objects may not be meaningful.\n\n\n\n7.1.2 Accuracy Assessment Methods in Remote Sensing Classification\n\n\n\n\nAccuracyAssessment. Source: Yuxi\n\n\n\n\n\nThe mind map shows that accuracy assessment methods in remote sensing classification can be categorized into four main categories: overall accuracy evaluation, confusion matrix evaluation, ROC curve evaluation, and sampling methods evaluation.\n\n\nThe first category, overall accuracy evaluation, includes two methods: overall accuracy (OA) and Kappa coefficient (Kappa). The former calculates the percentage of correctly classified pixels, while the latter takes into account the agreement between the classified map and the reference map.\n\n\nThe second category, confusion matrix evaluation, involves constructing a confusion matrix that summarizes the classification results. The two methods included in this category are confusion matrix (CM) and user’s accuracy and producer’s accuracy. The former provides a table that shows the number of correctly and incorrectly classified pixels for each class, while the latter evaluates the accuracy of each class individually.\n\n\nThe third category, ROC curve evaluation, involves using the ROC curve and AUC to evaluate the performance of a classifier. The ROC curve plots the true positive rate against the false positive rate, while AUC measures the area under the ROC curve.\n\n\nThe last category, sampling methods evaluation, includes three subcategories: stratified sampling, cross-validation, and bootstrapping. Stratified sampling involves dividing the study area into strata and sampling each stratum proportionally or non-proportionally. Cross-validation involves splitting the data into training and testing sets and evaluating the classifier’s performance on the testing set. Bootstrapping involves repeatedly sampling with replacement from the original dataset to generate new datasets and evaluate the classifier’s performance on each new dataset."
  },
  {
    "objectID": "week7.html#application",
    "href": "week7.html#application",
    "title": "\n7  week7\n",
    "section": "\n7.2 Application",
    "text": "7.2 Application\n\n7.2.1 Clssification Example\n\nSubPixel Analysis, Object-Based Image Analysis and Superpixel Analysis are performed on remote sensing images and produce some results： \n\nmy code link: https://code.earthengine.google.com/595af734bc6965a6f51a15aefd5a7dfb\n\n\n\n\nClassificationII. Source: Yuxi\n\n\n\n\n\n7.2.2 Clssification Application\nHerold et al. (2016) apply object-based image analysis and superpixel segmentation to map urban land cover from high-resolution aerial imagery. They compare their results to traditional per-pixel methods and find that their approach is more accurate in identifying complex urban features such as small parks and residential areas. The paper’s contribution is to provide a more efficient and accurate method for mapping urban land cover, which can aid in urban planning and management.\nKhan et al. (2013) apply subpixel analysis to hyperspectral imagery to map urban vegetation at the sub-pixel level. They compare their results to traditional pixel-based methods and find that their approach is more accurate in identifying vegetation in urban environments with low vegetation cover. The paper’s contribution is to provide a more accurate method for mapping urban vegetation, which can aid in urban planning and management.\nGoetz et al. (2018) apply object-based image analysis to map urban trees using LiDAR and multispectral imagery. They find that their approach is more accurate in identifying individual trees and their characteristics, such as height and crown size, compared to traditional pixel-based methods. The paper’s contribution is to provide a more accurate and detailed method for mapping urban trees, which can aid in urban planning and management, as well as in assessing urban forest ecosystems."
  },
  {
    "objectID": "week7.html#reflection",
    "href": "week7.html#reflection",
    "title": "\n7  week7\n",
    "section": "\n7.3 Reflection",
    "text": "7.3 Reflection\n\nAfter learning about subpixel analysis, object-based image analysis, superpixel analysis methods, as well as accuracy assessment methods in remote sensing classification, I have gained a deeper understanding of the challenges and opportunities in this field.\n\n\nSubpixel analysis is a method for improving the accuracy of land cover mapping by considering the fractional abundance of each class within a pixel. Object-based image analysis and superpixel analysis are two related methods that consider the spatial context of objects in an image and group pixels into objects based on their properties, respectively. These methods have advantages over traditional pixel-based classification, as they can better capture the spatial heterogeneity and complexity of the environment.\n\n\nAccuracy assessment is a critical step in remote sensing classification to evaluate the performance of the classification method. There are several metrics that can be used to assess accuracy, including overall accuracy, kappa coefficient, and user and producer accuracy. These metrics allow for the quantification of errors and uncertainties in the classification results, which is essential for making informed decisions and improving the accuracy of classification.\n\n\nFurthermore, I have learned about the applications of these methods in various fields, including urban and environmental monitoring, land use and land cover mapping, and disaster assessment. These applications have significant implications for understanding and addressing various environmental and societal issues.\n\n\nAs for my future work in improving urban environments, these methods can be useful in analyzing the distribution and changes of land use and land cover in urban areas, identifying urban green spaces, and monitoring urban growth and sprawl. By using accurate and reliable remote sensing classification methods, I can better understand the current state and trends of urban environments and develop targeted interventions to improve their sustainability and resilience.\n\nReferences:\nHerold, S., Atkinson, C., Stevens, J. L., & Doherty, D. C. (2016). Mapping Urban Land Cover Using Object-Based Image Analysis and Superpixel Segmentation. Remote Sensing, 8(6), 501.\nKhan, S. I., Mather, M. G., & Roberts, G. W. (2013). Subpixel Analysis of Hyperspectral Imagery for Urban Vegetation Mapping. Remote Sensing, 5(7), 3191-3218.\nGoetz, P. C., Sun, K., & Zolotoy, B. L. (2018). Object-Based Image Analysis for Mapping Urban Trees Using LiDAR and Multispectral Imagery. Remote Sensing, 10(2), 281."
  },
  {
    "objectID": "week1.html",
    "href": "week1.html",
    "title": "\n1  week1\n",
    "section": "",
    "text": "2 Week1 Getting started with remote sensing"
  },
  {
    "objectID": "week1.html#summary",
    "href": "week1.html#summary",
    "title": "\n1  week1\n",
    "section": "\n2.1 Summary",
    "text": "2.1 Summary\n\n2.1.1 Remote Sensing\n\n2.1.1.1 Definition and Applications of Remote Sensing\nRemote sensing refers to the technology and methods of observing and measuring the Earth’s surface using remote sensing platforms such as satellites and airplanes.\nRemote sensing technology is widely used in natural resource surveying, environmental monitoring, urban planning, agricultural production, weather forecasting, and other fields, providing important support for sustainable development of human society.\n\n\n\n\nSatellites. Source: Industry Tap\n\n\n\n\n\n2.1.1.2 sensors and electromagnetic waves\nTypes of Sensors Used in Remote Sensing\n\n\n\n\n\n\n\n\n\nSensor Type\nSensor Name\nEnergy Source\nMode\nInformation Obtained\n\n\n\nPassive\nLandsat\nReflected\nOptical\nHigh-resolution visible and near-infrared images\n\n\nPassive\nSentinel\nReflected\nOptical\nHigh-resolution visible and near-infrared images\n\n\nPassive\nMODIS\nEmitted\nThermal Infrared\nTemperature and thermal distribution information\n\n\nPassive\nASTER\nEmitted\nThermal Infrared\nTemperature and thermal distribution information\n\n\nActive\nALOS\nEmitted\nRadar\nSurface topography and moisture content information\n\n\nActive\nRADARSAT\nEmitted\nRadar\nSurface topography and moisture content information\n\n\nActive\nICESat\nEmitted\nLiDAR\nSurface elevation and 3D information\n\n\n\nElectromagnetic Waves in Remote Sensing Electromagnetic waves are the energy that travels through space in the form of electric and magnetic fields, including visible light, infrared radiation, microwave radiation, and radio waves. Different types of electromagnetic waves have different wavelengths and frequencies, and can interact with the Earth’s surface and atmosphere in different ways, allowing remote sensing sensors to measure various physical properties of the Earth’s surface and atmosphere. The electromagnetic spectrum is divided into different bands, including the visible band, near-infrared band, shortwave infrared band, thermal infrared band, microwave band, and radio band, each with its own applications in remote sensing.\nScatters In Resolution\n\nInteracting with Earth’s surface\n\n\n\n\nScattering. Source: Julien Chimot, from Bovensmann et al., 2011\n\n\n\n\n\n2.1.2 Remote Sensing Data\nRemote sensing data refers to information about the Earth’s surface and atmosphere that is obtained through remote sensing technologies, such as satellites, airplanes, drones, and ground-based sensors.\nThere are several ways to acquire remote sensing data. One approach is to purchase data from commercial providers who offer a range of satellite and aerial imagery products. Another option is to access publicly available data archives, such as those provided by the United States Geological Survey (USGS) or the European Space Agency (ESA).\n\n2.1.2.1 Types\n\n\n\n\n\n\n\nRemote Sensing Data\nDescription\nExamples\n\n\n\nOptical Image Data\nObtained using optical sensors on remote sensing platforms for high-resolution and multispectral imaging of the Earth’s surface.\nLandsat, Sentinel\n\n\nRadar Image Data\nObtained using radar sensors for imaging of the Earth’s surface, providing ground information at night and under cloud cover.\nALOS, RADARSAT\n\n\nLiDAR Data\nObtained using LiDAR sensors for scanning of the Earth’s surface, providing ground elevation and 3D information.\nLiDAR, ICESat\n\n\nThermal Infrared Data\nObtained using infrared sensors for thermal imaging of the Earth’s surface, providing temperature and thermal distribution information.\nMODIS, ASTER\n\n\n\n2.1.2.2 Resolution Type\n\n\nResolution Type\nDescription\n\n\n\nSpatial\nSize of the smallest detectable object\n\n\nSpectral\nNumber and width of spectral bands\n\n\nRadiometric\nNumber of bits used to represent the data\n\n\nTemporal\nTime interval between image captures\n\n\n\n2.1.3 Processing and Applications of Remote Sensing Data\nRemote sensing data needs to be pre-processed, such as radiometric correction, atmospheric correction, geometric correction, etc., to eliminate the influence of the sensor and environmental factors. The applications of remote sensing data include land cover classification, terrain measurement, vegetation index calculation, ocean and lake monitoring, urban construction planning, and other fields.\n\n2.1.4 Development and Trends of Remote Sensing Technology\nRemote sensing technology has gone through multiple stages from single-band to multi-band, low-resolution to high-resolution, and 2D to 3D. In the future, remote sensing technology will continue to develop in the direction of multi-source data fusion, high-performance computing, intelligent algorithm application, etc., to better serve the needs of various fields, such as climate change monitoring, disaster response, resource management, and so on."
  },
  {
    "objectID": "week1.html#application",
    "href": "week1.html#application",
    "title": "\n1  week1\n",
    "section": "\n2.2 Application",
    "text": "2.2 Application\nThis passage discusses a study that examines the relationship between land use and land cover and thermal environment in Belgrade.\nThe study used Landsat imagery from 1991 to 2019 to monitor spatiotemporal changes in green spaces and land surface temperature.\nThe results showed that there was a fluctuating trend in the normalized difference vegetation index (NDVI) and the normalized difference water index (NDWI), with the highest values recorded in 2019 indicating vegetation recovery in the last decade. There was a significant positive correlation between the spectral vegetation indices and the amount of precipitation during the growing season.\n\n\n\n\nMarković. Source: Monitoring of Spatiotemporal Change of Green Spaces in Relation to the Land Surface Temperature: A Case Study of Belgrade, Serbia Remote Sensing\n\n\n\n\nThe share of vegetated and bare land decreased by 11.74% during the study period, with the most intensive conversion of green and bare land into built-up land cover occurring in the first decade (1991–2000). The reduction in vegetation was associated with an increase in the land surface temperature, indicating a negative correlation between the change in the spectral vegetation indices and change in the LST.\n\n\n\n\nMarković. Source: Monitoring of Spatiotemporal Change of Green Spaces in Relation to the Land Surface Temperature: A Case Study of Belgrade, Serbia Remote Sensing\n\n\n\n\n\n\n\n\nMarković. Source: Monitoring of Spatiotemporal Change of Green Spaces in Relation to the Land Surface Temperature: A Case Study of Belgrade, Serbia Remote Sensing\n\n\n\n\n\n\n\n\nMarković. Source: Monitoring of Spatiotemporal Change of Green Spaces in Relation to the Land Surface Temperature: A Case Study of Belgrade, Serbia Remote Sensing\n\n\n\n\nThe study identified the municipalities that were the most affected in each decade, and the findings are relevant for actions targeting an improvement in urban thermal comfort and climate resilience. The passage also highlights the importance of green spaces in enhancing living conditions, contributing to adaptation to climate change, and biodiversity conservation. The study suggests that changes in green spaces in Belgrade were driven by fluctuations in climate factors, as well as human-induced changes in land use and land cover."
  },
  {
    "objectID": "week1.html#reflection",
    "href": "week1.html#reflection",
    "title": "\n1  week1\n",
    "section": "\n2.3 Reflection",
    "text": "2.3 Reflection\nLearning about remote sensing sensors, electromagnetic waves, and scattering has provided me with a better understanding of the principles behind remote sensing data acquisition. I now know that remote sensing data can be obtained from various sources, including Sentinel and Landsat satellites, and these data can be processed and analyzed using software such as SNAP, QGIS, and R.\nOne of the most important concepts I learned is spectral signature, which refers to the unique pattern of reflectance or absorption of electromagnetic radiation for different materials in various wavelengths. This knowledge can be applied to identify different land cover types, such as vegetation, bare land, and urban areas, which can be useful for urban and environmental monitoring and management.\nAnother useful concept I learned is color composites, which involve combining different spectral bands to create a false-color image that enhances certain features or properties of the scene. This technique can be used to highlight specific land cover types, such as vegetation and water, and can also be used to detect changes in land cover over time.\nMoreover, this knowledge can be useful for my future work in improving urban environments, such as analyzing the distribution of land cover and identifying urban green spaces. By utilizing remote sensing data and techniques, I can gain a better understanding of the current state and trends of urban environments and develop targeted interventions to improve their sustainability and livability.\nMarković M, Cheema J, Teofilović A, Čepić S, Popović Z, Tomićević-Dubljević J and Pause M 2021 Monitoring of Spatiotemporal Change of Green Spaces in Relation to the Land Surface Temperature: A Case Study of Belgrade, Serbia Remote Sensing 13 3846 Online: http://dx.doi.org/10.3390/rs13193846"
  },
  {
    "objectID": "week3.html",
    "href": "week3.html",
    "title": "\n3  week3\n",
    "section": "",
    "text": "Remote sensing data"
  },
  {
    "objectID": "week3.html#summary",
    "href": "week3.html#summary",
    "title": "\n3  week3\n",
    "section": "\n3.1 Summary",
    "text": "3.1 Summary\n\n3.1.1 Remote sensing image Correction\nRemote sensing image correction refers to the process of performing various correction operations on remote sensing images to make the images reflect surface features and object information more accurately. Correction operations usually include geometric correction, radiometric correction and atmospheric correction to remove distortions and errors in remotely sensed images due to the characteristics of the Earth’s surface and atmosphere.\nGeometric correction, radiometric correction, and atmospheric correction are commonly used techniques in remote sensing image correction. Geometric correction aims to remove geometric distortions caused by sensor position, terrain relief, and other factors, while radiometric correction aims to normalize the image pixel values to account for sensor and environmental conditions. Atmospheric correction is used to correct for atmospheric effects that can cause inaccuracies in remote sensing data.\n\n\n\n\nRemote Sensing Image Correction. Source: Industry Tap"
  },
  {
    "objectID": "week3.html#application",
    "href": "week3.html#application",
    "title": "\n3  week3\n",
    "section": "\n3.2 Application",
    "text": "3.2 Application"
  },
  {
    "objectID": "week3.html#reflection",
    "href": "week3.html#reflection",
    "title": "\n3  week3\n",
    "section": "\n3.3 Reflection",
    "text": "3.3 Reflection"
  }
]